# 2.2 Given a scenario, perform active reconnaissance.

## Enumeration

### Hosts

### Services

### Domains

### Users

### Uniform resource locators (URLs)

## Website reconnaissance

### Crawling websites

### Scraping websites

### Manual inspection of web links

**robots.txt**: Text file with instructions for search engine crawlers. It defines which areas of a website crawlers are allowed to search. However, this file does not protect against unauthorized access.

## Packet crafting

### Scapy

## Defense detection

### Load balancer detection

### Web application firewall (WAF) detection

### Antivirus

### Firewall

## Tokens

### Scoping

### Issuing

### Revocation

## Wardriving

## Network traffic

### Capture API requests and responses

### Sniffing

## Cloud asset discovery

## Third-party hosted services

## Detection avoidance
